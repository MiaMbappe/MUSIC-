{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music recommender system\n",
    "\n",
    "One of the most used machine learning algorithms is recommendation systems. A **recommender** (or recommendation) **system** (or engine) is a filtering system which aim is to predict a rating or preference a user would give to an item, eg. a film, a product, a song, etc.\n",
    "\n",
    "Which type of recommender can we have?   \n",
    "\n",
    "There are two main types of recommender systems: \n",
    "- Content-based filters\n",
    "- Collaborative filters\n",
    "  \n",
    "> Content-based filters predicts what a user likes based on what that particular user has liked in the past. On the other hand, collaborative-based filters predict what a user like based on what other users, that are similar to that particular user, have liked.\n",
    "\n",
    "We have previously developed a content-based recommendation system. Now, we'll look into collaborative filtering. \n",
    "\n",
    "### 2) Collaborative filters\n",
    "\n",
    "Collaborative Filters work with an interaction matrix, also called rating matrix. The aim of this algorithm is to learn a function that can predict if a user will benefit from an item - meaning the user will likely buy, listen to, watch this item.\n",
    "\n",
    "Among collaborative-based systems, we can encounter two types: **user-item** filtering and **item-item** filtering. \n",
    "\n",
    "*What algorithms do collaborative filters use to recommend new songs?* There are several machine learning algorithms that can be used in the case of collaborative filtering. Among them, we can mention nearest-neighbor, clustering, and matrix factorization.\n",
    "\n",
    "**K-Nearest Neighbors (kNN)** is considered the standard method when it comes to both user-based and item-based collaborative filtering approaches.\n",
    "  \n",
    "We'll go through the steps for generating a music recommender system using a k-nearest algorithm approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-604ebbd17881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the **[Million Song Dataset](http://millionsongdataset.com/)**, a freely-available collection of audio features and metadata for a million contemporary popular music tracks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two files that will be interesting for us. The first of them will give us information about the songs. Particularly, it contains the user ID, song ID and the listen count. On the other hand, the second file will contain song ID, title of that song, release, artist name and year. \n",
    "We need to merge these two DataFrames. For that aim, we'll use the `song_ID` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read userid-songid-listen_count\n",
    "song_info = pd.read_csv('https://static.turi.com/datasets/millionsong/10000.txt',sep='\\t',header=None)\n",
    "song_info.columns = ['user_id', 'song_id', 'listen_count']\n",
    "\n",
    "#Read song  metadata\n",
    "song_actual =  pd.read_csv('https://static.turi.com/datasets/millionsong/song_data.csv')\n",
    "song_actual.drop_duplicates(['song_id'], inplace=True)\n",
    "\n",
    "#Merge the two dataframes above to create input dataframe for recommender systems\n",
    "songs = pd.merge(song_info, song_actual, on=\"song_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save this dataset into a `csv file` so we have this available if there is any other recommendation system project we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.to_csv('songs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read this file into a new **DataFrame** that we'd call `df_songs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs = pd.read_csv('songs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, any data science or machine learning project starts with an exploratory data analysis (EDA). The aim of EDA is to understand and get insights on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first inspect the first rows of our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll check how many observions there are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get total observations\n",
    "print(f\"There are {df_songs.shape[0]} observations in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should perform some cleaning steps. But looking at the dataset, we can see that there is no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And most of the columns contain strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start exploring some characteristics of the dataset: \n",
    "\n",
    "- Unique songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique songs\n",
    "print(f\"There are {df_songs['title'].unique().shape[0]} unique songs in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unique artists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique artists\n",
    "unique_songs = df_songs['artist_name'].unique().shape[0]\n",
    "print(f\"There are {unique_songs} unique artists in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unique users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique users\n",
    "unique_users = df_songs['user_id'].unique().shape[0]\n",
    "print(f\"There are {unique_users} unique users in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go ahead and explore the popularity of songs and artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we determine which are the most popular songs? For this task, we'll count how many times each song appears. Note that while we are using  `listen_count`, we only care about the number of rows, we don't consider the number present in that row. This number represents how many times one user listen to the same song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count how many rows we have by song, we show only the ten more popular songs \n",
    "ten_pop_songs = df_songs.groupby('title')['listen_count'].count().reset_index().sort_values(['listen_count', 'title'], ascending = [0,1])\n",
    "ten_pop_songs['percentage']  = round(ten_pop_songs['listen_count'].div(ten_pop_songs['listen_count'].sum())*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_pop_songs = ten_pop_songs[:10]\n",
    "ten_pop_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ten_pop_songs['title'].tolist()\n",
    "counts = ten_pop_songs['listen_count'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.barplot(x=counts, y=labels, palette='Set3')\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular artist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next task, we'll count how many times each artist appears. Again, we'll count how many times the same artist appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count how many rows we have by artist name, we show only the ten more popular artist \n",
    "ten_pop_artists  = df_songs.groupby(['artist_name'])['listen_count'].count().reset_index().sort_values(['listen_count', 'artist_name'], \n",
    "                                                                                                ascending = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_pop_artists = ten_pop_artists[:10]\n",
    "ten_pop_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "labels = ten_pop_artists['artist_name'].tolist()\n",
    "counts = ten_pop_artists['listen_count'].tolist()\n",
    "sns.barplot(x=counts, y=labels, palette='Set2')\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first concatenate the name of the song with the artist so we have all the information in one column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate title and artist name. Assign it to new columns\n",
    "df_songs['song'] = df_songs['title'] + \"-\" + df_songs[\"artist_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen count by user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some other information from the feature `listen_count`. We will answer the folloging questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the maximum time the same user listen to a same song?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listen_counts = pd.DataFrame(df_songs.groupby('listen_count').size(), columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The maximum time the same user listened to the same songs was: {listen_counts.reset_index(drop=False)['listen_count'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many times on average the same user listen to a same song?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"On average, a user listen to the same song {df_songs['listen_count'].mean()} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the distribution of `listen_count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.boxplot(x='listen_count', data=df_songs)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the most frequent number of times a user listen to the same song?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listen_counts_temp = listen_counts[listen_counts['count'] > 50].reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.barplot(x='listen_count', y='count', palette='Set3', data=listen_counts_temp)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many songs does a user listen in average?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_user = df_songs.groupby('user_id')['song'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.distplot(song_user.values, color='orange')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A user listens to an average of {np.mean(song_user)} songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A user listens to an average of {np.median(song_user)} songs, with minimum {np.min(song_user)} and maximum {np.max(song_user)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a user listens in average to 27 songs. Even the maximum amount of songs listen by an user is 711, and we have 9567 songs in our dataset.\n",
    "\n",
    "So, not all user listen to all songs, so a lot of values in the `song x users` matrix are going to be zero. Thus, we’ll be dealing with extremely sparse data. \n",
    "\n",
    "*How sparse?* Let's check that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get how many values should it be if all songs have been listen by all users\n",
    "values_matrix = unique_users * unique_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substract the total values with the actural shape of the DataFrame songs\n",
    "zero_values_matrix = values_matrix - df_songs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The matrix of users x songs has {zero_values_matrix} values that are zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with such a sparse matrix, we'll take a lot of memory and resources. To make our life easier, let's just select all those users that have listen to at least 16 songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_ten_id = song_user[song_user > 16].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_id_more_ten = df_songs[df_songs['user_id'].isin(song_ten_id)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to work with a `scipy-sparse matrix` to avoid overflow and wasted memory. For that purpose, we'll use the `csr_matrix` function from `scipy.sparse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_features = df_song_id_more_ten.pivot(index='song_id', columns='user_id', values='listen_count').fillna(0)\n",
    "mat_songs_features = csr_matrix(df_songs_features.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the table `user x song`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_songs_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we know that we want to fit a model to predict songs. We'll use `NearestNeighbors` from the `sklearn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender:\n",
    "    def __init__(self, metric, algorithm, k, data, decode_id_song):\n",
    "        self.metric = metric\n",
    "        self.algorithm = algorithm\n",
    "        self.k = k\n",
    "        self.data = data\n",
    "        self.decode_id_song = decode_id_song\n",
    "        self.data = data\n",
    "        self.model = self._recommender().fit(data)\n",
    "    \n",
    "    def make_recommendation(self, new_song, n_recommendations):\n",
    "        return self._recommend(new_song, n_recommendations)\n",
    "    \n",
    "    def _recommender(self):\n",
    "        return NearestNeighbors(metric=self.metric, algorithm=self.algorithm, n_neighbors=self.k, n_jobs=-1)\n",
    "    \n",
    "    def _recommend(self, new_song, n_recommendations):\n",
    "        # Get the id of the recommended songs\n",
    "        recommendations = []\n",
    "        recommendation_ids = self._get_recommendations(new_song, n_recommendations)\n",
    "        # return the name of the song using a mapping dictionary\n",
    "        recommendations_map = self._map_indeces_to_song_title(recommendation_ids)\n",
    "        # Translate this recommendations into the ranking of song titles recommended\n",
    "        for i, (idx, dist) in enumerate(recommendation_ids):\n",
    "            recommendations.append(f'{i+1}: {recommendations_map[idx]}')\n",
    "        return recommendations\n",
    "                 \n",
    "    def _get_recommendations(self, new_song, n_recommendations):\n",
    "        # Get the id of the song according to the text\n",
    "        recom_song_id = self._matching(song=new_song)\n",
    "        # Start the recommendation process\n",
    "        print(f\"Starting the recommendation process for {new_song} ...\")\n",
    "        # Return the n neighbors for the song id\n",
    "        distances, indeces = self.model.kneighbors(self.data[recom_song_id], n_neighbors=n_recommendations+1)\n",
    "        return sorted(list(zip(indices.squeeze().tolist(), distances.squeeze().tolist())), key=lambda x: x[1])[:0:-1]\n",
    "    \n",
    "    def _map_indeces_to_song_title(self, recommendation_ids):\n",
    "        # get reverse mapper\n",
    "        return {song_id: song_title for song_title, song_id in self.decode_id_song.items()}\n",
    "    \n",
    "    def _fuzzy_matching(self, song):\n",
    "        \"\"\"\n",
    "        return the closest match via fuzzy ratio. \n",
    "\n",
    "        Parameters\n",
    "        ----------    \n",
    "        mapper: dict, map movie title name to index of the movie in data\n",
    "        fav_movie: str, name of user input movie\n",
    "\n",
    "        verbose: bool, print log if True\n",
    "        Return\n",
    "        ------\n",
    "        index of the closest match\n",
    "        \"\"\"\n",
    "        match_tuple = []\n",
    "        # get match\n",
    "        for title, idx in mapper.items():\n",
    "            ratio = fuzz.ratio(title.lower(), fav_movie.lower())\n",
    "            if ratio >= 60:\n",
    "                match_tuple.append((title, idx, ratio))\n",
    "        # sort\n",
    "        match_tuple = sorted(match_tuple, key=lambda x: x[2])[::-1]\n",
    "        if not match_tuple:\n",
    "            print('Oops! No match is found')\n",
    "            return\n",
    "        if verbose:\n",
    "            print('Found possible matches in our database: {0}\\n'.format([x[0] for x in match_tuple]))\n",
    "        return match_tuple[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_id_song = {\n",
    "    song: i for i, song in \n",
    "    enumerate(list(df_songs.set_index('song_id').loc[movie_user_mat.index].title))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Recommender(metric='cosine', algorithm='brute', n_neighbors=20, data=mat_songs_features, decode_id_song=decode_id_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_recommendations = model.make_recommendation('I believe in miracles', n_recommendation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
